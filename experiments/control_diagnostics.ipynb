{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Control-Theoretic Diagnostics\n",
        "\n",
        "We treat prompts as control inputs and logit entropy as an observability signal. Using the same miniature Transformer trained on country–capital data, we compute:\n",
        "\n",
        "- **Control projection:** how strongly each prompt moves the hidden state along the reference direction (relative to a neutral baseline).\n",
        "- **Logit entropy:** entropy of the model’s output distribution on the final token, serving as an observability proxy.\n",
        "\n",
        "Higher control projection + lower entropy indicates a prompt that both steers the system and provides confident observations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = [\n",
        "    \"<pad>\", \"<sos>\", \"<eos>\", \"The\", \"the\", \"capital\", \"of\", \"is\", \"What\", \"?\", \"It\", \".\", \"city\",\n",
        "    \"France\", \"Paris\", \"Germany\", \"Berlin\", \"Italy\", \"Rome\", \"Spain\", \"Madrid\",\n",
        "    \"Portugal\", \"Lisbon\", \"Greece\", \"Athens\", \"UK\", \"London\", \"Russia\", \"Moscow\",\n",
        "    \"Japan\", \"Tokyo\", \"China\", \"Beijing\", \"India\", \"New\", \"Delhi\", \"Brazil\", \"Brasilia\",\n",
        "    \"Canada\", \"Ottawa\", \"Australia\", \"Canberra\", \"Egypt\", \"Cairo\", \"Turkey\", \"Ankara\"\n",
        "]\n",
        "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
        "\n",
        "countries_capitals = {\n",
        "    \"France\": \"Paris\", \"Germany\": \"Berlin\", \"Italy\": \"Rome\", \"Spain\": \"Madrid\",\n",
        "    \"Portugal\": \"Lisbon\", \"Greece\": \"Athens\", \"UK\": \"London\", \"Russia\": \"Moscow\",\n",
        "    \"Japan\": \"Tokyo\", \"China\": \"Beijing\", \"India\": \"New Delhi\", \"Brazil\": \"Brasilia\",\n",
        "    \"Canada\": \"Ottawa\", \"Australia\": \"Canberra\", \"Egypt\": \"Cairo\", \"Turkey\": \"Ankara\"\n",
        "}\n",
        "\n",
        "sentences = []\n",
        "for country, capital in countries_capitals.items():\n",
        "    sentences.append(f\"The capital of {country} is {capital} .\")\n",
        "    sentences.append(f\"What is the capital of {country} ? It is {capital} .\")\n",
        "    sentences.append(f\"The capital city of {country} is {capital} .\")\n",
        "\n",
        "sentences *= 3\n",
        "\n",
        "\n",
        "def tokenize(sentence: str):\n",
        "    tokens = sentence.replace(\".\", \" .\").split()\n",
        "    input_ids = [word_to_idx[\"<sos>\"]] + [word_to_idx.get(tok, word_to_idx[\"<pad>\"]) for tok in tokens]\n",
        "    target_ids = [word_to_idx.get(tok, word_to_idx[\"<pad>\"]) for tok in tokens] + [word_to_idx[\"<eos>\"]]\n",
        "    return torch.tensor(input_ids), torch.tensor(target_ids)\n",
        "\n",
        "\n",
        "class CapitalDataset(Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.examples = [tokenize(s) for s in sentences]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    inputs = pad_sequence([b[0] for b in batch], batch_first=True, padding_value=word_to_idx[\"<pad>\"])\n",
        "    targets = pad_sequence([b[1] for b in batch], batch_first=True, padding_value=word_to_idx[\"<pad>\"])\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "dataset = CapitalDataset(sentences)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = torch.triu(torch.ones(sz, sz)) == 1\n",
        "    mask = mask.transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "class SimpleLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=48, nhead=3, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(512, d_model))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=192)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        seq_len = src.size(1)\n",
        "        emb = emb + self.pos_embedding[:seq_len, :]\n",
        "        emb = emb.transpose(0, 1)\n",
        "        mask = generate_square_subsequent_mask(seq_len)\n",
        "        hidden = self.encoder(emb, mask=mask)\n",
        "        hidden = hidden.transpose(0, 1)\n",
        "        return self.proj(hidden)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleLM(len(vocab)).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx[\"<pad>\"])\n",
        "\n",
        "epochs = 8\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.reshape(-1, len(vocab)), targets.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def prompt_stats(prompt: str):\n",
        "    model.eval()\n",
        "    prompt = prompt.replace(\".\", \" .\")\n",
        "    tokens = prompt.split()\n",
        "    ids = torch.tensor([[word_to_idx[\"<sos>\"]] + [word_to_idx.get(tok, word_to_idx[\"<pad>\"]) for tok in tokens]], device=device)\n",
        "    emb = model.embedding(ids) * math.sqrt(model.d_model)\n",
        "    seq_len = ids.size(1)\n",
        "    emb = emb + model.pos_embedding[:seq_len, :]\n",
        "    emb = emb.transpose(0, 1)\n",
        "    mask = generate_square_subsequent_mask(seq_len).to(device)\n",
        "    hidden = model.encoder(emb, mask=mask)\n",
        "    hidden = hidden.transpose(0, 1)\n",
        "    mean_hidden = hidden.mean(dim=1).squeeze(0).cpu()\n",
        "    logits = model.proj(hidden)\n",
        "    final_logits = logits[:, -1, :]\n",
        "    probs = torch.softmax(final_logits, dim=-1)\n",
        "    entropy = (-probs * torch.log(probs + 1e-9)).sum().item()\n",
        "    return mean_hidden, entropy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_prompt = \"The capital of\"\n",
        "reference_prompt = \"The capital of France is Paris. The capital of Germany is Berlin. The capital of Italy is Rome. The capital of Spain is Madrid. Therefore, the capital of Spain is Madrid.\"\n",
        "\n",
        "prompt_variants = {\n",
        "    \"Direct\": \"The capital of Spain is\",\n",
        "    \"Chain-of-Thought\": \"Let's reason carefully. Spain is a country in Europe. Its well-known capital city is Madrid. So the capital of Spain is\",\n",
        "    \"Scaffolded\": \"Instructions: recall European capitals. Fact: France -> Paris, Germany -> Berlin, Italy -> Rome, Spain -> Madrid. Answer: The capital of Spain is\",\n",
        "    \"Noisy\": \"Some people think Barcelona is the capital. Others mention Madrid. The capital of Spain is\",\n",
        "    \"Self-check\": \"Answer the question, then verify it. Step 1: Spain is in Europe. Step 2: Its capital is Madrid. Verification: Does that match known facts? Yes. The capital of Spain is\",\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_hidden, baseline_entropy = prompt_stats(baseline_prompt)\n",
        "reference_hidden, reference_entropy = prompt_stats(reference_prompt)\n",
        "ref_dir = F.normalize(reference_hidden - baseline_hidden, dim=0)\n",
        "\n",
        "rows = []\n",
        "for label, prompt in prompt_variants.items():\n",
        "    vec, entropy = prompt_stats(prompt)\n",
        "    control_vec = vec - baseline_hidden\n",
        "    control_proj = torch.dot(F.normalize(control_vec, dim=0), ref_dir).item()\n",
        "    rows.append({\n",
        "        \"variant\": label,\n",
        "        \"control_projection\": control_proj,\n",
        "        \"entropy\": entropy,\n",
        "    })\n",
        "\n",
        "ctrl_df = pd.DataFrame(rows).sort_values(\"control_projection\", ascending=False)\n",
        "ctrl_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "sns.scatterplot(data=ctrl_df, x=\"control_projection\", y=\"entropy\", hue=\"variant\", s=120)\n",
        "plt.axhline(reference_entropy, color=\"gray\", linestyle=\"--\", label=\"Reference entropy\")\n",
        "plt.title(\"Control vs. Observability Proxies\")\n",
        "plt.xlabel(\"Control projection (cosine)\")\n",
        "plt.ylabel(\"Logit entropy (lower is better)\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "# Save figure for the paper\n",
        "fig_path = \"../figures/control_observability_scatter.png\"\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.scatterplot(data=ctrl_df, x=\"control_projection\", y=\"entropy\", hue=\"variant\", s=120)\n",
        "plt.axhline(reference_entropy, color=\"gray\", linestyle=\"--\")\n",
        "plt.xlabel(\"Control projection (cosine)\")\n",
        "plt.ylabel(\"Logit entropy\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(fig_path, dpi=200)\n",
        "plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prompts with feedback (self-check, scaffolded) both push strongly along the reference direction and maintain low entropy, meaning the system is controllable and observable. Noisy prompts barely project onto the goal direction and produce higher entropy, signaling drift.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
