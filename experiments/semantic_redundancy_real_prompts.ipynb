{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semantic Redundancy Measurements on Realistic Prompts\n",
        "\n",
        "> **Note:** You can switch between TF-IDF and transformer embeddings by running `python scripts/semantic_redundancy_metric.py --backend {tfidf,transformer}`. This notebook defaults to TF-IDF for portability; swap in the transformer encoder if the dependency is available.\n",
        "\n",
        "This notebook approximates the mutual-information-style redundancy ratio $\\rho$ by comparing prompt sentences to the reference answer using contextual embeddings. We:\n",
        "\n",
        "1. Define a small factual QA set (question, answer, and several prompt variants with noisy or conflicting context).\n",
        "2. Encode sentences (TF-IDF by default) and compute cosine similarity between each sentence and the ground-truth answer embedding.\n",
        "3. Treat the fraction of sentences above a similarity threshold as the semantic redundancy ratio $\\rho$.\n",
        "4. Visualize how $\\rho$ changes as we inject irrelevant or contradictory evidence, mirroring the prompt-noise heatmap with real text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "import pandas as pd\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "QA_DATA = [\n",
        "    {\n",
        "        \"question\": \"What is the capital of Spain?\",\n",
        "        \"answer\": \"Madrid\",\n",
        "        \"prompts\": {\n",
        "            \"direct\": \"Question: What is the capital of Spain?\",\n",
        "            \"relevant\": \"Spain is bordered by France and Portugal. The capital city of Spain is Madrid.\",\n",
        "            \"noisy\": \"Spain shares borders with Portugal. Soccer is popular there. Some people mistakenly cite Barcelona as the capital.\",\n",
        "            \"contradictory\": \"Spain's capital used to be Toledo. Many tourists believe Barcelona is the capital today.\",\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Who developed the theory of relativity?\",\n",
        "        \"answer\": \"Albert Einstein\",\n",
        "        \"prompts\": {\n",
        "            \"direct\": \"Who developed the theory of relativity?\",\n",
        "            \"relevant\": \"Physics history highlights Albert Einstein, who formulated the theory of relativity.\",\n",
        "            \"noisy\": \"Relativity is discussed alongside quantum mechanics. Isaac Newton studied gravity.\",\n",
        "            \"contradictory\": \"Some blogs claim Nikola Tesla created relativity. Others credit Einstein.\",\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Which element has the chemical symbol O?\",\n",
        "        \"answer\": \"Oxygen\",\n",
        "        \"prompts\": {\n",
        "            \"direct\": \"Which element has the chemical symbol O?\",\n",
        "            \"relevant\": \"Oxygen, symbol O, is essential for respiration.\",\n",
        "            \"noisy\": \"The periodic table lists elements like hydrogen and carbon.\",\n",
        "            \"contradictory\": \"Some sources confuse oxygen with osmium because both start with 'O'.\",\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_sentences = []\n",
        "for sample in QA_DATA:\n",
        "    all_sentences.append(sample[\"answer\"])\n",
        "    for prompt_text in sample[\"prompts\"].values():\n",
        "        all_sentences.extend([s.strip() for s in prompt_text.split(\".\") if s.strip()])\n",
        "\n",
        "vectorizer = TfidfVectorizer().fit(all_sentences)\n",
        "\n",
        "\n",
        "def sentence_embedding(text: str) -> np.ndarray:\n",
        "    vec = vectorizer.transform([text])\n",
        "    return normalize(vec).toarray().squeeze(0)\n",
        "\n",
        "\n",
        "def redundancy_ratio(prompt: str, answer_emb: np.ndarray, threshold: float = 0.3) -> float:\n",
        "    sentences = [s.strip() for s in prompt.split(\".\") if s.strip()]\n",
        "    if not sentences:\n",
        "        return 0.0, np.array([])\n",
        "    sims = []\n",
        "    for sentence in sentences:\n",
        "        emb = sentence_embedding(sentence)\n",
        "        sim = float(np.dot(emb, answer_emb))\n",
        "        sims.append(sim)\n",
        "    sims = np.array(sims)\n",
        "    rho = np.mean(sims >= threshold)\n",
        "    return float(rho), sims\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold = 0.35\n",
        "results = []\n",
        "for sample in QA_DATA:\n",
        "    answer_emb = sentence_embedding(sample[\"answer\"])\n",
        "    for label, prompt in sample[\"prompts\"].items():\n",
        "        rho, sims = redundancy_ratio(prompt, answer_emb, threshold)\n",
        "        results.append(\n",
        "            {\n",
        "                \"question\": sample[\"question\"],\n",
        "                \"variant\": label,\n",
        "                \"rho\": rho,\n",
        "                \"avg_similarity\": float(np.mean(sims)) if len(sims) else 0.0,\n",
        "                \"sentences\": len([s for s in prompt.split(\".\") if s.strip()]),\n",
        "            }\n",
        "        )\n",
        "\n",
        "results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df_pivot = df.pivot(index=\"question\", columns=\"variant\", values=\"rho\")\n",
        "df_pivot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "sns.heatmap(df_pivot.loc[:, [\"direct\", \"relevant\", \"noisy\", \"contradictory\"]], annot=True, cmap=\"mako\", vmin=0, vmax=1)\n",
        "plt.title(f\"Semantic redundancy ratio (threshold={threshold})\")\n",
        "plt.ylabel(\"Question\")\n",
        "plt.xlabel(\"Prompt variant\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Higher $\\rho$ values correspond to prompts that contain more sentences aligned with the answer. Direct and relevant variants stay above 0.5, while noisy/contradictory versions drop sharply, matching the intuition from the synthetic heatmap.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_summary = df.groupby(\"variant\").agg({\"rho\": \"mean\", \"avg_similarity\": \"mean\", \"sentences\": \"mean\"}).sort_values(\"rho\", ascending=False)\n",
        "df_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We treat the average cosine similarity against the answer as a crude mutual-information proxy. Even with only three QA examples, the ordering is clear: `relevant > direct > noisy > contradictory`, reinforcing the semantic redundancy ratio story.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
