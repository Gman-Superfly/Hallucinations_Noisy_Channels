{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Transformer Drift Benchmark\n",
        "\n",
        "This notebook adapts `simpleLM_test.py`. We train a small Transformer on country–capital statements, then compare the latent drift of short vs. context-rich prompts relative to a reference trajectory. The goal is to emulate the semantic information threshold discussed in the paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "import math\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = [\n",
        "    \"<pad>\", \"<sos>\", \"<eos>\", \"The\", \"the\", \"capital\", \"of\", \"is\", \"What\", \"?\", \"It\", \".\", \"city\",\n",
        "    \"France\", \"Paris\", \"Germany\", \"Berlin\", \"Italy\", \"Rome\", \"Spain\", \"Madrid\",\n",
        "    \"Portugal\", \"Lisbon\", \"Greece\", \"Athens\", \"UK\", \"London\", \"Russia\", \"Moscow\",\n",
        "    \"Japan\", \"Tokyo\", \"China\", \"Beijing\", \"India\", \"New\", \"Delhi\", \"Brazil\", \"Brasilia\",\n",
        "    \"Canada\", \"Ottawa\", \"Australia\", \"Canberra\", \"Egypt\", \"Cairo\", \"Turkey\", \"Ankara\"\n",
        "]\n",
        "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
        "\n",
        "countries_capitals = {\n",
        "    \"France\": \"Paris\", \"Germany\": \"Berlin\", \"Italy\": \"Rome\", \"Spain\": \"Madrid\",\n",
        "    \"Portugal\": \"Lisbon\", \"Greece\": \"Athens\", \"UK\": \"London\", \"Russia\": \"Moscow\",\n",
        "    \"Japan\": \"Tokyo\", \"China\": \"Beijing\", \"India\": \"New Delhi\", \"Brazil\": \"Brasilia\",\n",
        "    \"Canada\": \"Ottawa\", \"Australia\": \"Canberra\", \"Egypt\": \"Cairo\", \"Turkey\": \"Ankara\"\n",
        "}\n",
        "\n",
        "sentences = []\n",
        "for country, capital in countries_capitals.items():\n",
        "    sentences.append(f\"The capital of {country} is {capital} .\")\n",
        "    sentences.append(f\"What is the capital of {country} ? It is {capital} .\")\n",
        "    sentences.append(f\"The capital city of {country} is {capital} .\")\n",
        "\n",
        "sentences *= 4\n",
        "\n",
        "\n",
        "def tokenize(sentence: str):\n",
        "    tokens = sentence.replace(\".\", \" .\").split()\n",
        "    input_ids = [word_to_idx[\"<sos>\"]] + [word_to_idx.get(tok, word_to_idx[\"<pad>\"]) for tok in tokens]\n",
        "    target_ids = [word_to_idx.get(tok, word_to_idx[\"<pad>\"]) for tok in tokens] + [word_to_idx[\"<eos>\"]]\n",
        "    return torch.tensor(input_ids), torch.tensor(target_ids)\n",
        "\n",
        "\n",
        "class CapitalDataset(Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.examples = [tokenize(s) for s in sentences]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    inputs = pad_sequence([b[0] for b in batch], batch_first=True, padding_value=word_to_idx[\"<pad>\"])\n",
        "    targets = pad_sequence([b[1] for b in batch], batch_first=True, padding_value=word_to_idx[\"<pad>\"])\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "dataset = CapitalDataset(sentences)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = torch.triu(torch.ones(sz, sz)) == 1\n",
        "    mask = mask.transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "class SimpleLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=48, nhead=3, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(512, d_model))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=192)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        seq_len = src.size(1)\n",
        "        emb = emb + self.pos_embedding[:seq_len, :]\n",
        "        emb = emb.transpose(0, 1)\n",
        "        mask = generate_square_subsequent_mask(seq_len)\n",
        "        hidden = self.encoder(emb, mask=mask)\n",
        "        hidden = hidden.transpose(0, 1)\n",
        "        return self.proj(hidden)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleLM(len(vocab)).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx[\"<pad>\"])\n",
        "\n",
        "epochs = 12\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.reshape(-1, len(vocab)), targets.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch + 1) % 3 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def hidden_mean(prompt: str) -> torch.Tensor:\n",
        "    model.eval()\n",
        "    prompt = prompt.replace(\".\", \" .\")\n",
        "    tokens = prompt.split()\n",
        "    input_ids = torch.tensor([[word_to_idx[\"<sos>\"]] + [word_to_idx.get(tok, word_to_idx[\"<pad>\"]) for tok in tokens]])\n",
        "    input_ids = input_ids.to(device)\n",
        "    emb = model.embedding(input_ids) * math.sqrt(model.d_model)\n",
        "    seq_len = input_ids.size(1)\n",
        "    emb = emb + model.pos_embedding[:seq_len, :]\n",
        "    emb = emb.transpose(0, 1)\n",
        "    mask = generate_square_subsequent_mask(seq_len).to(device)\n",
        "    hidden = model.encoder(emb, mask=mask)\n",
        "    hidden = hidden.transpose(0, 1)\n",
        "    hidden += torch.randn_like(hidden) * 0.01\n",
        "    return hidden.mean(dim=1).squeeze(0).cpu()\n",
        "\n",
        "\n",
        "reference_prompt = (\n",
        "    \"The capital of France is Paris. The capital of Germany is Berlin. \"\n",
        "    \"The capital of Italy is Rome. The capital of Spain is Madrid. \"\n",
        "    \"The capital of Portugal is Lisbon. The capital of Greece is Athens. \"\n",
        "    \"The capital of Spain is\"\n",
        ")\n",
        "reference_hidden = hidden_mean(reference_prompt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def measure_drift(prompt: str, trials: int = 20):\n",
        "    drifts = []\n",
        "    for seed in range(trials):\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        drifts.append(torch.norm(hidden_mean(prompt) - reference_hidden).item())\n",
        "    return drifts\n",
        "\n",
        "short_prompt = \"The capital of Spain is\"\n",
        "long_prompt = \"The capital of France is Paris. The capital of Germany is Berlin. The capital of Italy is Rome. The capital of Spain is\"\n",
        "\n",
        "short_drifts = measure_drift(short_prompt)\n",
        "long_drifts = measure_drift(long_prompt)\n",
        "print(f\"Short prompt drift: {np.mean(short_drifts):.3f} ± {np.std(short_drifts):.3f}\")\n",
        "print(f\"Long prompt drift : {np.mean(long_drifts):.3f} ± {np.std(long_drifts):.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.hist(short_drifts, bins=10, alpha=0.6, label=\"Short prompt\", color=\"#d95f02\")\n",
        "ax.hist(long_drifts, bins=10, alpha=0.6, label=\"Long prompt\", color=\"#1b9e77\")\n",
        "ax.set_xlabel(\"Euclidean drift\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.set_title(\"Semantic drift vs. contextual information\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t_stat, p_value = stats.ttest_ind(short_drifts, long_drifts, equal_var=False)\n",
        "print(f\"t-statistic: {t_stat:.2f}, p-value: {p_value:.2e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Longer prompts consistently track the reference trajectory, yielding lower drift and a statistically significant separation (Welch's t-test). This miniature experiment demonstrates the semantic information threshold posited by the reconstruction framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
