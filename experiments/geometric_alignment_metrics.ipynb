{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Geometric Alignment Metrics\n",
        "\n",
        "We reuse the miniature Transformer from `simpleLM_drift.ipynb` to measure how different prompt styles align with a reference latent manifold. After training the model on country–capital statements, we:\n",
        "\n",
        "1. Define several prompts (direct question, Chain-of-Thought, scaffolded instructions, noisy context).\n",
        "2. Extract the mean hidden state for each prompt.\n",
        "3. Compare each hidden state to a “gold” reference vector and report cosine similarities.\n",
        "\n",
        "This approximates controllability (how far a prompt moves us toward the concept basin) and provides a geometric alignment metric for prompt design.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = [\n",
        "    \"<pad>\", \"<sos>\", \"<eos>\", \"The\", \"the\", \"capital\", \"of\", \"is\", \"What\", \"?\", \"It\", \".\", \"city\",\n",
        "    \"France\", \"Paris\", \"Germany\", \"Berlin\", \"Italy\", \"Rome\", \"Spain\", \"Madrid\",\n",
        "    \"Portugal\", \"Lisbon\", \"Greece\", \"Athens\", \"UK\", \"London\", \"Russia\", \"Moscow\",\n",
        "    \"Japan\", \"Tokyo\", \"China\", \"Beijing\", \"India\", \"New\", \"Delhi\", \"Brazil\", \"Brasilia\",\n",
        "    \"Canada\", \"Ottawa\", \"Australia\", \"Canberra\", \"Egypt\", \"Cairo\", \"Turkey\", \"Ankara\"\n",
        "]\n",
        "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
        "\n",
        "countries_capitals = {\n",
        "    \"France\": \"Paris\", \"Germany\": \"Berlin\", \"Italy\": \"Rome\", \"Spain\": \"Madrid\",\n",
        "    \"Portugal\": \"Lisbon\", \"Greece\": \"Athens\", \"UK\": \"London\", \"Russia\": \"Moscow\",\n",
        "    \"Japan\": \"Tokyo\", \"China\": \"Beijing\", \"India\": \"New Delhi\", \"Brazil\": \"Brasilia\",\n",
        "    \"Canada\": \"Ottawa\", \"Australia\": \"Canberra\", \"Egypt\": \"Cairo\", \"Turkey\": \"Ankara\"\n",
        "}\n",
        "\n",
        "sentences = []\n",
        "for country, capital in countries_capitals.items():\n",
        "    sentences.append(f\"The capital of {country} is {capital} .\")\n",
        "    sentences.append(f\"What is the capital of {country} ? It is {capital} .\")\n",
        "    sentences.append(f\"The capital city of {country} is {capital} .\")\n",
        "\n",
        "sentences *= 3\n",
        "\n",
        "\n",
        "def tokenize(sentence: str):\n",
        "    tokens = sentence.replace(\".\", \" .\").split()\n",
        "    input_ids = [word_to_idx[\"<sos>\"]] + [word_to_idx.get(tok, word_to_idx[\"<pad>\"]) for tok in tokens]\n",
        "    target_ids = [word_to_idx.get(tok, word_to_idx[\"<pad>\"]) for tok in tokens] + [word_to_idx[\"<eos>\"]]\n",
        "    return torch.tensor(input_ids), torch.tensor(target_ids)\n",
        "\n",
        "\n",
        "class CapitalDataset(Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.examples = [tokenize(s) for s in sentences]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    inputs = pad_sequence([b[0] for b in batch], batch_first=True, padding_value=word_to_idx[\"<pad>\"])\n",
        "    targets = pad_sequence([b[1] for b in batch], batch_first=True, padding_value=word_to_idx[\"<pad>\"])\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "dataset = CapitalDataset(sentences)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = torch.triu(torch.ones(sz, sz)) == 1\n",
        "    mask = mask.transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "class SimpleLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=48, nhead=3, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(512, d_model))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=192)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        seq_len = src.size(1)\n",
        "        emb = emb + self.pos_embedding[:seq_len, :]\n",
        "        emb = emb.transpose(0, 1)\n",
        "        mask = generate_square_subsequent_mask(seq_len)\n",
        "        hidden = self.encoder(emb, mask=mask)\n",
        "        hidden = hidden.transpose(0, 1)\n",
        "        return self.proj(hidden)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleLM(len(vocab)).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx[\"<pad>\"])\n",
        "\n",
        "epochs = 8\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.reshape(-1, len(vocab)), targets.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def hidden_state(prompt: str) -> torch.Tensor:\n",
        "    model.eval()\n",
        "    prompt = prompt.replace(\".\", \" .\")\n",
        "    tokens = prompt.split()\n",
        "    ids = torch.tensor([[word_to_idx[\"<sos>\"]] + [word_to_idx.get(tok, word_to_idx[\"<pad>\"]) for tok in tokens]], device=device)\n",
        "    emb = model.embedding(ids) * math.sqrt(model.d_model)\n",
        "    seq_len = ids.size(1)\n",
        "    emb = emb + model.pos_embedding[:seq_len, :]\n",
        "    emb = emb.transpose(0, 1)\n",
        "    mask = generate_square_subsequent_mask(seq_len).to(device)\n",
        "    hidden = model.encoder(emb, mask=mask)\n",
        "    hidden = hidden.transpose(0, 1)\n",
        "    return hidden.mean(dim=1).squeeze(0).cpu()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"The capital of Spain is\"\n",
        "reference_prompt = \"The capital of France is Paris. The capital of Germany is Berlin. The capital of Italy is Rome. The capital of Spain is Madrid. Therefore, the capital of Spain is Madrid.\"\n",
        "reference_hidden = hidden_state(reference_prompt)\n",
        "\n",
        "prompt_variants = {\n",
        "    \"Direct\": \"The capital of Spain is\",\n",
        "    \"Chain-of-Thought\": \"Let's reason carefully. Spain is a country in Europe. Its well-known capital city is Madrid. So the capital of Spain is\",\n",
        "    \"Scaffolded\": \"Instructions: recall European capitals. Fact: France -> Paris, Germany -> Berlin, Italy -> Rome, Spain -> Madrid. Answer: The capital of Spain is\",\n",
        "    \"Noisy\": \"Some people think Barcelona is the capital. Others mention Madrid. The capital of Spain is\",\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "rows = []\n",
        "ref_norm = F.normalize(reference_hidden, dim=0)\n",
        "for label, prompt in prompt_variants.items():\n",
        "    vec = hidden_state(prompt)\n",
        "    similarity = torch.dot(F.normalize(vec, dim=0), ref_norm).item()\n",
        "    rows.append({\"variant\": label, \"cos_similarity\": similarity})\n",
        "\n",
        "alignment_df = pd.DataFrame(rows).sort_values(\"cos_similarity\", ascending=False)\n",
        "alignment_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 3))\n",
        "sns.barplot(data=alignment_df, x=\"cos_similarity\", y=\"variant\", palette=\"viridis\")\n",
        "plt.title(\"Latent alignment vs. prompt style\")\n",
        "plt.xlabel(\"Cosine similarity to reference manifold\")\n",
        "plt.ylabel(\"Prompt variant\")\n",
        "plt.xlim(0, 1)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chain-of-Thought and scaffolded prompts land much closer to the reference manifold than direct or noisy prompts. This aligns with the controllability framing: richer control inputs (reasoning steps, explicit instructions) steer the hidden state into the desired basin, while noisy context drifts away.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
